1500+ distinct Microservices at Monzo (Banking, 3 million customers), Please do not take this as permission to create 1,500 microservices for your product that would do just fine with 5, Context matters.
* All services run in self-managed Kubernetes on AWS EC2
* Number of pods ~6,500
* Every line is an enforced network rule allowing traffic
* 150+ engineers, tightly scoped, almost all GO, per microservice LOC 500 to 10K, use Github's code owners feature, so each pull request must be reviewed by the team that owns the affected service(s) before it can be merged
* Over the last 30 day an average request from the edge took 58ms to serve (which involves lots of internal requests)
* 100% monorepo on GitHub, own tooling for deployment
* Apart from a very small number of services written in other languages for reasons, all 1500 services are simple Go HTTP listeners and/or Kafka consumers, running all the time.
* Orchestrating microservices to orchestrate calls between multiple services
* ~5 in the center that have a high density of connections, Configuration / Authentication / Authorization & Event Scheduling
* Network policies (Calico) are autogenerated based on 'rule files' that represent pairings eg (A -> B.)
* Custom envoy-based enforcement between Kubernetes pods. Since K8s deployment manifests don’t describe traffic, static analysis is over golang., Network policies mainly, it could be done without envoy.
* distributed transactions, etcd for distributed locking. Locks are scoped to an individual service and the resource (e.g. user ID) and are held for the duration of the request. Most requests are retried internally to recover from partial failures.
* Self managed, apparently have more etcd throughput than etcds own stress test benchmarks.
* APM, Prometheus and Jaeger. Cassandra(One keyspace per service), Kubernetes (Self Managed) and Envoy for resilient to failing services. RPC and events via Kafka and NSQ. Etcd for distributed locking.
* Sensitive customer data in Cassandra, enforced using Calico
* Cassandra with eventual consistency, doesn’t that mess things up for financial data? run it with higher consistency guarantees if desired. Our core financial ledger tables generally have a variety of systems overseeing them and checking for consistency
* Aggregations, read from a small number of Cassandra partitions and aggregate under a lock within a single request., to aggregate large amounts of data (e.g. for analytics or financial reporting), use BigQuery.
* firehose run on NSQ, all services publish events to the firehose, with a set of services consuming and shipping that out to BigQuery, it does seem strange to ship to a different cloud provider, but their offering is very good.
* on-call, 16 volunteers from various teams on the out of hours rotation, in hours the platform team does triage and then teams resolve
* No downtime during full deploy all services (few days)
* Blog post coming soon with more specifics
